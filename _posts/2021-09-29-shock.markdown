---
layout: post
title:  "엔진 최적화 여정 ( 캐시 미스 ) ( 작성 중 )"
date:   2000-09-30
categories: ComputerScience ComputerGraphics
---

만들던 게임 엔진 성능 측정을 하던 중 예전에 만든 뷰프러스텀 컬링이 가장 많은 연산 시간을 차지하고 있다는 사실을 알았다.           
<img width="509" alt="20211009155327" src="https://user-images.githubusercontent.com/33873804/136647571-06e14054-17b4-45b1-870b-eedc864880cc.png">         
맨 위에 보이는 커널단 파일을 제외하고는 CullBlockEntityJob 함수가 가장 많은 CPU 시간을 차지하는데 이 함수가 [예전에 만든 뷰프러스텀 컬링 연산](https://github.com/SungJJinKang/EveryCulling/tree/main/CullingModule/ViewFrustumCulling)을 수행하는 함수이다.      
컬링 연산을 하는데 공간 분할이 아닌 Brute Force한 방법을 채택하고 있기 때문에 오브젝트 수가 늘어날 수록 컬링 연산 시간도 함께 늘어난다. Brute Force하게 모든 오브젝트에 대해 컬링 여부를 연산하는 대신 멀티스레드를 활용하여 연산 속도를 높였다. 실제로도 서브 스레드들에 컬링 Job을 던져두고 다른 그래픽 관련 상태 설정을 한 후 본격적으로 드로우 콜 API를 호출할 시점이 되면 서브 스레드들에 맡겨둔 컬링 Job은 완전히 끝나있다.       
그래서 사실 현재는 이 서브 스레드가 수행하는 뷰프러스텀 컬링 연산이 메인 스레드에서 렌더링을 하는데 성능 영향이 크지 않기 때문에 ( 메인 스레드가 서브 스레드들의 컬링 연산이 완전히 끝나기를 기다릴 일이 거의 없다 ) 문제가 되지 않지만, 향후에 추가할 CPU단에서 수행하는 [Masked SW Occlusion Culling](https://github.com/SungJJinKang/EveryCulling/tree/doom_engine_version/CullingModule/MaskedSWOcclusionCulling)도 멀티스레드로 연산하기 때문에 지금이라도 멀티스레드 컬링 연산의 연산량을 줄여둘 필요가 있다.       

-----------------------
      
내가 좋아하는 배틀필드를 만드는데 사용되었던 [프로스트바이트 엔진에서 구현한 것](https://www.slideshare.net/DICEStudio/culling-the-battlefield-data-oriented-design-in-practice)을 참고해서 거의 비슷하게 구현하려 노력하였다.       
간략히 소개하자면 **핵심은 SoA를 통한 캐시 효율 극대화, SIMD 명령어 사용**을 꼽을 수 있다.        
코드에 대해 간략하게 설명하자면 뷰 프러스텀 컬링을 연산데 사용되는 오브젝트들의 데이터를 모아둔 엔티티 블록이라는 것이 있는데 이 엔티티 블록 하나는 대략 50개 정도의 오브젝트에 대한 위치, 컬링 연산을 위한 데이터를 가지고 있다.          
그럼 오브젝트가 1000개라면 총 20개의 엔티티 블록이 있는 것이다.           
중요한건 오브젝트들의 **데이터가 Configous 하게 배치**되었다는 것이다.
```
Object1 Position | Object2 Position | Object3 Position | Object4 Position ...
```
이런 식으로 Array로 오브젝트들의 위치, 바운딩 Sphere의 반지름 총 16바이트의 오브젝트에 대한 데이터를 연속되게 배치하였다.       
이를 통해 **SIMD 연산 ( AVX1 )으로 빠르게 연산**하고 ( 한번 연산당 2개의 오브젝트에 대한 컬링 여부 알 수 있음 ), **캐시 적중률을 극대화**하였다.    
또한 **각의 스레드가 하나의 엔티티 블록을 전담하여서 계산**하기 때문에 **스레드간 Data Race가 없고**, 스레드간 **[캐시 동기화](https://sungjjinkang.github.io/computerscience/2021/04/06/cachecoherency.html)를 최소화**해주었다.    
또한 혹시 있을 캐시 동기화를 막기 위해 각각의 엔티티 블록은 **캐시 라인에 align**되게 셋팅하였다.       
자세한건 [코드](https://github.com/SungJJinKang/EveryCulling/tree/main/CullingModule/ViewFrustumCulling)를 보기 바란다.                                    

어쨌든 당연히 멀티스레드로 짰으니 빠르겠지 하고 생각하고 있었다.        
그런데 문뜻 테스트를 해보고 싶었다. 싱글스레드와 비교해서 얼마나 빠를지.        
( 참고로 테스트한 컴퓨터는 8코어 16스레드 CPU가 장착되어있다. )         

16000개의 오브젝트를 그리는 약간은 극단적인 상황을 가정하고 멀티스레드로 동작하는 코드와 싱글스레드롣 동작하는 코드의 성능을 비교하였다.      
렌더링 ( 컬링부터 그래픽스 API 호출을 전부 포함하는 )을 시작하고 끝날 때까지의 시간을 측정하였다.     
**싱글스레드**로 동작하는 코드는 **20ms** 소요되었다.     
**6개의 스레드** ( 메인스레드 1 + 서브스레드 5 )로 멀티스레드로 동작하는 코드는 **12ms** 소요되었다.     
**16개의 스레드** ( 메인스레드 1 + 서브스레드 15 )로 멀티스레드로 동작하는 코드는 **12ms** 소요되었다.       

여기서 우리는 **두가지 사실**을 알 수 있다.     
**멀티스레드로 동작하는 코드가** 싱글스레드로 동작하는 코드에 비해 **8ms 빨랐는데** 사실 **테스트 상황이 16000개의 오브젝트를 그리는 극단적인 상황이라는 사실**이다. 실제로 개발된 게임에서 있을법한 상황에서는 그 차이가 훨씬 적을 것이다.      
( 멀티스레드로 스레드 풀에 작업할 것들을 던져두고 메인스레드에서는 여러 다른 일들을 처리하게 해서 최대한 메인 스레드가 서브스레드가 작업을 다 끝내기를 기다리는 시간을 최소화하였다. 실제로 메인스레드는 거의 ( 99.99% ) 서브스레드들을 기다릴일이 없다. )                    

실제로 개발한 게임이 있을법한 개수의 오브젝트로 테스트를 하면 멀티스레드로 뷰프러스텀 연산을 할때와 싱글스레드로 할때 별차이가 없을 것이다.     
**내가 느끼기에는 왠만한 게임에서는 뷰프러스텀 컬링을 멀티스레드로 처리해봤자 얻을 수 있는 성능향상이 거의 제로에 가까울 것이다.**     

이러한 테스트 결과를 얻으니 약간은 현타가 왔다.....       
그래도 개발을 하면서 배운 것이 많으니 어찌 어찌 만족하고 있다.     

다음으로 알 수 있었던 사실은 **16스레드나 6스레드나 성능면에서 차이가 제로에 가깝다**는 것이다.       
이는 멀티스레드에 대해 조금이라도 아는 사람이라면 이유를 알 것이다.          

그 이유로는 크게 **스레드 컨텍스트 스위칭 비용**과 **캐시 효율 저하**가 있을 것 같다. 다른 이유도 있으니 [이 글](https://megayuchi.com/2018/02/15/%eb%a9%80%ed%8b%b0%ec%bd%94%ec%96%b4%ec%a7%80%ec%9b%90%ec%9d%84-%ec%95%88%ed%95%b4%ec%84%9c-%ea%b2%8c%ec%9e%84%ec%9d%b4-%eb%8a%90%eb%a0%a4/)을 참고하기 바란다.        

어차피 코어는 한정되어 있고 결국 스레드들은 코어를 돌아가면서 점유해서 사용할 것이다.       
그러다 보니 **스레드가 연산을 계속하다 컨텍스트 스위칭이 되고 그 스레드에 다른 코어가 배정이 되면 그 전에 축적해두었던 캐시가 다 쓸모 없어지게 되고 엄청난 캐시미스가 발생**하는 것이다. ( 밑에서 실제로 이 사례를 적나라하게 보여줄 것이다. )       

스레드 컨텍스트 스위칭의 경우 솔직히 확인할 방법이 없지만 ( 사실 있는지 없는지 모른다 ), 스레드의 개수가 16개나 된다고 생각했을 때 16개의 스레드가 돌아가면서 코어를 사용하려하는데 당연히 스레드 컨텍스트 스위칭이 상당할 것이다.        

사실 싱글스레드를 멀티스레드로 바꾸어도 성능이 크게 좋아지지 않는다는 것은 이미 잘 알고있었다.       
대형 게임 회사들도 괜히 코어를 한 두개만 갈구는 것이 아니다. 그만큼 멀티 스레드로 코드를 짠다고 해도 얻을 수 있는 성능 향상이 크지 않기 때문이다.       
[이 영상](https://youtu.be/pBrDKnJ6vjQ)을 보면 대형 회사들의 최신 게임들을 플레이 할 때 코어, 하드웨어 스레드의 개수에 따라 얼마나 성능 향상을 가질 수 있는지를 보여주는데, 코어가 아무리 많아도 별 차이 없다.      
그만큼 멀티 스레드로 코드를 작성하여도 큰 성능 향상을 얻기가 매우 힘들다는 것이다.      

----------------------

그렇지만 포기하고 싶지 않았다.     
조금이라도 멀티스레드 뷰 프러스텀 컬링을 빠르게 하기 위한 방법을 찾기로 했다.        
우선 어떤 부분이 문제가 되는지 알아야했다.        
나는 아마 멀티 스레드로 동작을 하면서 캐시 미스가 중요한 성능 저하의 원인 중 하나라고 생각하였다.       
캐시의 중요성에 대해서는 여러 CPPCON 영상들을 통해 잘 알고 있었다.          

AMD uProf CPU 프로파일러로 분석에 들어갔다.      
일단 **멀티 스레드 코드에서 캐시 미스가 엄청나다**는 사실을 발견했다.      

먼저 몇가지 사진들을 보여주겠다.        

먼저 **싱글 스레드** 상황에서의 프로파일링 결과이다.        
![싱글스레드](https://user-images.githubusercontent.com/33873804/135473368-11f713eb-6a9b-4d92-9bec-958f20c4aa00.png)       

다음은 **6스레드 멀티 스레드** 상황에서의 프로파일링 결과이다.        
![멀티스레드-6스레드](https://user-images.githubusercontent.com/33873804/135473361-ebf81514-0f30-4619-a21c-42412209b139.png)           

한눈에 봐도 멀티스레드 상황에서 캐시 미스가 많이 발생한다.     

특히 ~.SetPosition() 함수의 캐시 미스율이 엄청났다.    
이 코드는 게임 엔진 내의 오브젝트들의 Transform 컴포넌트에서 Position 데이터를 복사하여 컬링 연산을 하기 위한 데이터 영역으로 옮기는 코드이다. ( 컬링 연산에서 Transform 컴포넌트의 Position 데이터를 바로 활용하지 않는 이유는 오브젝트들의 Position 데이터를 SoA 형태로 배치하여 SIMD 연산으로 여러 오브젝트의 Position 데이터에 대한 컬링 여부를 빠르게 계산하기 위함이다. )            
6812번 데이터를 로드하는 동안 6125번 캐시 미스가 발생하였다. 이로 인한 로드 Latency 또한 매우 컸다.      
LCL_CACHE_MISS라는 캐시 미스를 나타내는데 캐시 미스가 발생하여 L3 공유 캐시 ( L3 캐시는 모든 코어가 공유하는 캐시이다 ), 혹은 DRAM에서 데이터를 로드하였다는 것이다. 당연히 느리다.          

어셈블리를 보면 확연히 알 수 있다.         
<img width="818" alt="20211009154658" src="https://user-images.githubusercontent.com/33873804/136648254-b81a91c1-3dcf-43a3-b1e8-23eb01e18963.png">         

vmovups xmm0, [ax+00000080h] 부분이 Transform 컴포넌트에서 Position 데이터를 레지스터로 읽어오는 명령어 코드이다. 그리고 맨 오른쪽에서 두번째 숫자가 로드시 캐시 미스 횟수이고 그 오른쪽은 로드시 캐시 히트 횟수이다. 미스율이 확연히 높은 것을 알 수 있다.       

이것을 줄이는 것이 목표이다...           


그래서 일단은 스레드 Affinity를 설정해보기로 하였다.           
스레드 Affinity는 스레드가 점유하는 논리적 CPU 코어를 설정하는 OS단 기능이다.         
일반적으로는 스레드들은 하드웨어상의 모든 논리적 CPU 코어 ( 물리적 코어뿐만 아니라 하드웨어 스레드도 포함한다. 하드웨어 스레드가 무엇이냐하면 그냥 하이퍼스레딩 생각하면 된다 )를 사용할 수 있다.       
그러다보니 멀티스레드 환경에서 여러 스레드가 특정 코어를 마구 사용하다보니 캐시가 오염 ( 이전에 캐시로 로드하였던 캐시 라인이 다음 캐시 레벨로 축출되는 현상 )될 가능성도 높아진다.       
그래서 스레드에게 특정 코어만을 접근할 수 있게 설정을 하여서 캐시 오염을 막을 수 있다.      
캐시 오염뿐만 아니라 빈번한 스레드 컨테스트 스위치도 방지할 수 있다.          

그러나 결과는 똑같았다.      
스레드 컨테스트 스위치가 문제는 아니였나보다....       
