---
layout: post
title:  "멀티스레드 뷰 프로스텀 컬링 최적화 여정 ( 캐시 미스 줄이기! - 데이터 Oriented 디자인 )"
date:   2021-09-29
categories: ComputerScience ComputerGraphics
---

결과는 충격적이었다.         
기껏 고생해서 멀티스레드로 코드를 돌렸는데 싱글 스레드로 돌린 것과 차이가 없었다.     
각 스레드가 연산을 할 때마다 thread id를 찍어보니 이상하게도 스레드들이 동시에 연산하지 않고 한 스레드가 끝나면 다음 스레드가, 그리고 그 다음 스레드가 이런 식으로 병렬적으로 연산이 되고 있지 않은 것을 발견했다.     

문제를 분석하기 시작했다.          
우선 코어간 캐시 동기화가 너무 빈번하게 발생한 것이 큰 원인일 것이라고 생각하고 AMD uProf CPU 프로파일러를 다운 받고 분석에 들어갔다.       

역시나 문제는 캐시 동기화였다.   

스레드간의 연산한 블록 횟수를 세기 위해 사용한 atomic 변수가 문제였다.        
atomic변수에 값을 쓸 때마다 다른 코어와 동기화가 발생하니 매번 공유 메모리까지 flush가 됬던 것이다.         

**동기화 자체가 문제가 아니라 M, E 상태를 가지고 있는 코어가 flush를 해주기까지 다른 코어들은 다 놀고 있어야한다는 것이 가장 큰 문제였다..** ( 캐시 Coherent에 대해 잘 알지 못하다면 [이 글](https://sungjjinkang.github.io/computerscience/2021/04/06/cachecoherency.html)을 읽기 바란다. )     
이러니 아토믹 변수에 대해 캐시 라인을 점유하고 있던 스레드만 계속 돌고 해당 캐시라인이 완전히 공유 메모리로 flush될 때까지 어느 정도 시간이 걸리니 그 동안 이 스레드만 동작을 계속하던거였다. 그 후 다른 코어가 해당 아토믹 변수의 캐시 라인을 M, E 상태로 점유하면 또 이 코어만 동작을 했던 것이다.                           
그러니 스레드들이 동시에 돌지 않고 한 스레드가 

불필요한 아토믹 연산이 총 4번 있었는데 이 때마다 각 코어가 서로 서로 캐시 flush를 요청하는 것이다.....         
그래서 이 아토믹 연산을 1번으로 줄였다.          
드디어 모든 코어가 함께 일을 하기 시작한다!!!          


현재도 atomic 변수를 한번은 사용하는 형태이다.      
이후에는 스레드의 인덱스에 따라 블록을 따로 계산하게 만들 예정이다.     
그러니깐 총 5개의 스레드가 있으면 0번째 스레드는 0, 5, 10, 15. 1번째 스레드는 1, 6, 11, 16 이렇게 동작하게 해서 atomic 변수의 사용을 완전히 배제할 것이다.       
물론 한 스레드가 자신에게 배정된 블록을 다 연산하면 다른 스레드의 연산을 빼앗아오는 것까지 구현할 것이다.
```
struct alignas(64) alignedAtomic
{
    std::atomic<size_t> atomic;
}

std::array<alignedAtomic, 스레드 풀 내의 스레드 개수> alignedAtomicValue;
```
위와 같이 각각의 atomic 변수의 캐시 라인을 완전히 분리해주어서 코어간의 atomic 변수 Cache coherency를 완전히 막을 것이다.        


차근 차근 프로파일러와 함께 살펴보겠다.      
```
const math::Vector3& renderedObjectPos = renderer->GetTransform()->GetPosition();

entityBlock->mPositions[entityIndex].SetPosition(*reinterpret_cast<const culling::Vector3*>(&renderedObjectPos));
entityBlock->mPositions[entityIndex].SetBoundingSphereRadius(worldRadius);
```
우선 첫번째 부분이다.        


그리고 나중에는 Position 데이터는 따로 풀링을 해야되겠다.         
매 프레임 오브젝트의 Posion 데이터에 2번 이상 접근을 하는데 매 오브젝트마다 캐시 미스가 발생하고 있다.       
그래서 미리 풀링을 해서 사용을 해야겠다.        

현재는 **Position 데이터가 힙 영역 여기 저기 파편화되어 있다보니 캐시의 혜택을 얻지 못하고 있다.**             
그래서 **많은 개수의 Position 데이터들을 배열로 미리 할당해서 각 오브젝트의 Position 데이터가 연속되게 배치**를 해야겠다.    

위에 보이는 LOAD_LATENCY가 모두 




```
struct mPosition
{
    culling::Vector3 Position;
    float BoundingSphereRadius;
};

for (size_t entityIndex = 0; entityIndex < entityCountInBlock; entityIndex++)
{
    doom::Renderer* renderer = static_cast<doom::Renderer*>(currentEntityBlock->mRenderer[entityIndex]);
    currentEntityBlock->mPositions[entityIndex].Position = renderer->GetTransform()->GetPosition();

}

for (size_t entityIndex = 0; entityIndex < entityCountInBlock; entityIndex++)
{
    doom::Renderer* renderer = static_cast<doom::Renderer*>(currentEntityBlock->mRenderer[entityIndex]);

    const float worldRadius = renderer->doom::ColliderUpdater<doom::physics::Sphere>::GetWorldCollider()->mRadius;

    currentEntityBlock->mPositions[entityIndex].BoundingSphereRadius = worldRadius;

}

vs

for (size_t entityIndex = 0; entityIndex < entityCountInBlock; entityIndex++)
	{
		doom::Renderer* renderer = static_cast<doom::Renderer*>(currentEntityBlock->mRenderer[entityIndex]);
		const float worldRadius = renderer->doom::ColliderUpdater<doom::physics::Sphere>::GetWorldCollider()->mRadius;

		currentEntityBlock->mPositions[entityIndex].Position = renderer->GetTransform()->GetPosition();
		currentEntityBlock->mPositions[entityIndex].BoundingSphereRadius = worldRadius;

	}
```

둘 중 어떤 코드가 빠를까?             
밑의 코드가 훨씬 더 빠르다.         
왜냐면 위의 코드의 경우 첫번째 루프에서 저장한 캐시를 두번째 루프에서 활용하지 못한다.     
그래서 두번째 루프에서 쓰기 동작을 할 때 또 다시 mPositions 데이터를 L1 캐시로 캐시 라인을 읽어와야한다.     

반면 아래 코드의 경우 첫번째 루프에서 Position과 BoundingSphereRadius를 쓰기 떄문에 처음 Position에 쓰기 동작을 수행하며 읽어왔던 캐시 덕분에 BoundingSphereRadius에 쓸 때는 곧 바로 쓸 수 있다.     
