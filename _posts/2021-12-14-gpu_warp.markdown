---
layout: post
title:  "GPU Warp ( 워프 ) ( GPU 하드웨어 관점에서 SoA 방식이 더 빠른 이유 )"
date:   2021-12-14
categories: ComputerScience ComputerGraphics
---

이 글에서는 GPU로 데이터를 전송할 때 SoA형태로 데이터를 전송하는 것이 더 빠른 하드웨어적인 이유에 대해 알아보겠다.         

얼마전 GPU로 전송되는 메쉬 데이터의 레이아웃을 바꾸었다.      
AoS에서 SoA로 바꾸었다.     

기존의 코드는 아래와 같이 AoS의 형태였다.       

```c++   
struct VetexData
{
    Vector3 mVertex;
    Vector3 mTexCoord;
    Vector3 mNormal;
    Vector3 mTangent;
    Vector3 mBitangent;
};
std::vector<VertexData> MeshData;
```

아래는 바꾼 SoA 버전의 코드이다.           

```c++     
struct MeshData
{
    Vector3* mVertex; // array
    Vector3* mTexCoord; // array
    Vector3* mNormal; // array
    Vector3* mTangent; // array
    Vector3* mBitangent; // array
};
```

우선 SoA 버전이 빠른 이유는 **캐시** 때문이다.    

아래의 경우 ( AoS ) 버전의 경우 메시 데이터 중 Vertex 데이터를 읽는다고 생각해보자.        
첫번째 Vertex 데이터를 읽은 후 다음 Vertex 데이터를 읽었을 때 다음 Vertex 데이터는 60 바이트 떨어져있다.      
캐시라인 사이즈는 64바이트니 **다음 Vertex 데이터를 읽기 위해서는 반드시 캐시 미스가 발생**한다. ( 다음 캐시라인이 캐시에 없는 경우 )         
이는 엄청난 성능 저하를 가져온다.        
```
Vertex(12바이트) mTexCoord(12바이트) mNormal(12바이트) mTangent(12바이트) mBitangent(12바이트) Vertex(12바이트) mTexCoord(12바이트) mNormal(12바이트) mTangent(12바이트) mBitangent(12바이트) 
```

반면 SoA 버전을 보자.      
다음 Vertex 데이터를 읽는 경우에도 매우 높은 확률로 같은 캐시 라인에 속해 있을 가능성이 높으니 당연히 빠르게 데이터를 가져올 수 있고 높은 성능 향상을 얻을 수 있다.             
```
Vertex(12바이트) Vertex(12바이트) Vertex(12바이트) Vertex(12바이트) Vertex(12바이트) Vertex(12바이트) Vertex(12바이트) Vertex(12바이트)...
```

---------------------

그럼 GPU의 관점에서는 왜 SoA 버전이 더 빠를까?        
당연히 GPU에도 캐시가 있으니 캐시 미스가 덜 발생한다는점도 맞다.         
또 다른 이유는 바로 **Warp(워프)** 때문이다.            
GPU는 여러 Scalar Processor가 모인 SM(Streaming Processor)라는 코어들의 집합으로 구성되어 있다.       
이 SM은 여러개의 Scalar Processor ( 논리 연산자, 부동 소수점 장치들로 구성됨 )로 구성되어 있고, 이 Scalar Processor는 공유메모리, L1 캐시, 텍스쳐 메모리 등을 공유한다.        

아래의 사진은 GPU내의 SM의 구조를 보여준다. 이러한 SM이 GPU 내부에 수십개 있다고 생각하면 된다.        
![image11-601x1024](https://user-images.githubusercontent.com/33873804/145867752-3793f9c2-7212-47b5-8226-4a00e6dcbc3f.jpg)              

그리고 이 SM 내부에는 여러 Warp들이 있다.             
이 Warp는 간단히 설명하면 **여러 ( 32 or 64 ) 스레드들의 집합**이라고 생각하면 된다.        
GPU는 흔히 **SIMT** ( Single Instruction, Multiple Thread )라고 부르는 이유도 이 때문이다.       
여러 스레드들 ( Warp )들은 항상 동일한 명령어를 수행해야한다.              

이 Warp의 가장 큰 특징은 **동시점에 동일한 연산을 수행**해야한다는 것이다.       
이 **Warp내의 모든 스레드들은 항상 연산을 수행할 때 동일한 명령어를 수행**해야한다.           

예를 들어보자.        
아래의 코드는 픽셀 쉐이더의 일부분으로 distance 변수의 값에 따라 FragColor 변수에 다른 색을 저장한다.        

```c++
layout (location = 0) in float distance; 
out vec4 FragColor;

void main()
{     
    if( distance > 0.5 )
    {
        FragColor = vec4(1.0, 0.0, 0.0, 1.0);
    }
    else
    {
        FragColor = vec4(0.0, 0.0, 1.0, 1.0);
    }
}
```
분명 위에서 Warp내의 스레드들은 동일한 시점에 동일한 명령어를 수행해야한다고 말했다. Fragment 32개가 모여서 해당 32개의 Fragment에 대해 픽셀 쉐이딩을 수행하는 스레드들이 하나의 Warp를 이루고 있다고 생각해보자.                   
32개의 Fragment 모두가 위의 두 **분기점 ( if, else )** 중 하나의 분기로 선택이 된다면 다행이다.        
그런데 **만약 32개의 Fragment 중 한 Fragment가 다른 분기로 간다면** 어떻게 될까?            
**다른 31개의 Fragment Shading Thread들도 이 다른 하나의 분기를 따라가야한다.** 왜냐면 Warp 내에서는 동일한 시점에 동일한 연산을 수행해야하니깐 다른 31개의 Thread들도 다른 일을 못하고 똑같이 다른 하나의 분기를 따라 똑같이 명령어를 수행해야한다. ( 다만 Commit은 하지않는다. )         
               
      
그럼 다시 SoA 얘기로 돌아와보자.    
아래의 Vertex 쉐이딩의 경우를 보자.        

```c++
layout (location = 0) in vec3 vertex; 
void main()
{
    ~~~   
}
```                
             

만약 AoS의 경우 어떠한가.        

```c++
Vertex(12바이트) mTexCoord(12바이트) mNormal(12바이트) mTangent(12바이트) mBitangent(12바이트) Vertex(12바이트) mTexCoord(12바이트) mNormal(12바이트) mTangent(12바이트) mBitangent(12바이트) 
```
자 32개의 Vertex에 대해 Vertex 쉐이딩을 수행하는 스레드들 32개가 모여 Warp를 이루고있다.        
Vertex Shading을 수행하려면 당연히 Vertex 데이터를 가져와야한다.       
32개의 스레드 중 첫번째 스레드가 첫번째 Vertex를 가져왔다. 자 이제 연산을 시작해도될까?      
아니다. **Warp의 속성상 Warp내의 다른 스레드들도 함께 동일한 연산을 수행해야하니 두번째 스레드도 Vertex 쉐이딩을 시작할 준비가 되어야 Warp가 연산을 시작할 수 있다.**            
두번째 스레드가 Vertex를 가져오려고 한다. 근데 두번째 Vertex가 너무 멀리 떨어져있다. SoA 데이터 레이아웃을 가지고 있다 보니 **첫번째 스레드가 Vertex를 가져오면서 함께 캐시라인을 캐시로 가져왔지만 두번째 Vertex가 멀리 떨어져있어 또 새로운 캐시라인을 가져와야한다.**            
매우 느리다.....           
세번째 스레드는? 또 새로운 캐시라인을 가져와야한다...... 너무 느리다......            
**Warp 내의 모든 스레드 ( 32 or 64 )들이 각자가 연산할 Vertex 데이터를 모두 가져올 때까지 Warp 내의 모든 스레드들은 Vertex Shading 시작도 하지 못한다. 그냥 다른 스레드가 Vertex 데이터를 가져오기를 완료할 때까지 기다리고 있어야한다.** ( 자기는 이미 Vertex를 가져와서 연산을 시작할 수 있음에도 Warp의 특성상 다른 스레드를 기다려야하는 것이다. )           

여기서 오는 성능하락이 경우에 따라 매우 클 수 있다.        
그렇기 때문에 GPU로 데이터를 전송할 때는 함께 사용될 데이터들을 최대한 응집시킬 ( SoA ) 필요가 있는 것이다.      
심지어 CUDA 프로그래밍에서는 데이터를 캐시라인에 Align되게 할당해서 최대한 Warp 내의 스레드들이 동일한 캐시라인에서 데이터를 가져오게 만든다.      





참고하면 좋은 글들 : [https://mkblog.co.kr/2018/10/13/gpgpu-series-5-scheduling-thread-blocks/](https://mkblog.co.kr/2018/10/13/gpgpu-series-5-scheduling-thread-blocks/), [https://mkblog.co.kr/2018/12/27/gpgpu-series-6-thread-block-to-warps/](https://mkblog.co.kr/2018/12/27/gpgpu-series-6-thread-block-to-warps/), [https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/), [https://blog.cherryservers.com/everything-you-need-to-know-about-gpu-architecture](https://blog.cherryservers.com/everything-you-need-to-know-about-gpu-architecture)                                        